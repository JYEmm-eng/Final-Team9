{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "G8lScXt_WJcR",
        "jGQxzaJrWaxz",
        "Cf3AQcX4Wkdl",
        "MLpUtNdIWqBm",
        "yfSfULGIXCcT",
        "x1c2A8buXFC0",
        "INbPqAWXXUfg"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JYEmm-eng/Final-Team9/blob/main/9%ED%8C%80_%ED%8C%8C%EC%9D%B4%EB%84%90_%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TMDB Data 전처리 과정"
      ],
      "metadata": {
        "id": "G8lScXt_WJcR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#revenue, budget = null, 0인 행 필터링(파일 크기 이슈로 VSCODE 통해 진행)\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "file_path = 'TMDB_movie_dataset_v11.csv'\n",
        "# 새로 저장할 파일의 이름을 정합니다.\n",
        "output_file_path = 'filteredTMDB.csv'\n",
        "\n",
        "# 첫 번째 덩어리인지 확인하기 위한 변수를 만듭니다. (헤더를 한 번만 저장하기 위함)\n",
        "is_first_chunk = True\n",
        "\n",
        "# 파일을 10만 행씩 잘라서 순차적으로 처리\n",
        "chunk_iterator = pd.read_csv(\n",
        "    file_path,\n",
        "    chunksize=100000,\n",
        "    encoding='utf-8',\n",
        "    low_memory=False\n",
        ")\n",
        "\n",
        "print(f\"필터링을 시작합니다. 결과는 '{output_file_path}' 파일에 저장됩니다...\")\n",
        "\n",
        "# 각 덩어리(chunk)를 순회하며 조건에 맞는 행을 찾고 저장합니다.\n",
        "for i, chunk in enumerate(chunk_iterator):\n",
        "    chunk['budget'] = pd.to_numeric(chunk['budget'], errors='coerce').fillna(0)\n",
        "    chunk['revenue'] = pd.to_numeric(chunk['revenue'], errors='coerce').fillna(0)\n",
        "\n",
        "    # --- 핵심 조건 ---\n",
        "    valid_rows = chunk[(chunk['budget'] != 0)]\n",
        "\n",
        "    # 찾은 데이터가 있을 경우에만 저장 작업을 수행합니다.\n",
        "    if not valid_rows.empty:\n",
        "        if is_first_chunk:\n",
        "            # 첫 번째 덩어리는 헤더와 함께 새 파일로 저장합니다.\n",
        "            valid_rows.to_csv(\n",
        "                output_file_path,\n",
        "                index=False,\n",
        "                encoding='utf-8-sig'\n",
        "            )\n",
        "            is_first_chunk = False  # 다음부터는 이 코드를 실행하지 않도록 변경\n",
        "        else:\n",
        "            # 두 번째 덩어리부터는 헤더 없이 기존 파일에 내용을 추가합니다.\n",
        "            valid_rows.to_csv(\n",
        "                output_file_path,\n",
        "                mode='a',\n",
        "                header=False,\n",
        "                index=False,\n",
        "                encoding='utf-8-sig'\n",
        "            )\n",
        "\n",
        "    print(f\"{i+1}번째 덩어리 처리 중...\")\n",
        "\n",
        "print(\"\\n--- 작업 완료 ---\")\n",
        "print(f\"'{output_file_path}' 파일이 생성되었습니다.\")"
      ],
      "metadata": {
        "id": "ok-EMJb6WI90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oU5KrLGMU1wK"
      },
      "outputs": [],
      "source": [
        "#TMDB API 크롤링(VSCODE 통해 진행)\n",
        "\n",
        "import pandas as pd\n",
        "import requests\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- 1. API 키 설정 ---\n",
        "# TMDB에서 발급받은 본인의 API 키를 여기에 입력하세요.\n",
        "API_KEY = 'API_KEY' # 예: 'a1b2c3d4e5f6a1b2c3d4e5f6a1b2c3d4'\n",
        "\n",
        "def get_credits(movie_id):\n",
        "    \"\"\"\n",
        "    주어진 movie_id로 감독(Director)과 상위 5명의 배우(Cast) 이름을 찾아 반환하는 함수\n",
        "    \"\"\"\n",
        "    time.sleep(0.05) # API 서버에 부담을 주지 않기 위해 잠시 대기\n",
        "    url = f\"https://api.themoviedb.org/3/movie/{int(movie_id)}/credits?api_key={API_KEY}&language=en-US\"\n",
        "\n",
        "    director = None\n",
        "    cast = None\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, timeout=10)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "\n",
        "            # 감독 찾기\n",
        "            for member in data.get('crew', []):\n",
        "                if member.get('job') == 'Director':\n",
        "                    director = member.get('name')\n",
        "                    break # 첫 번째 감독만 찾고 종료\n",
        "\n",
        "            # 상위 5명 배우 이름 가져오기\n",
        "            cast_list = [actor.get('name') for actor in data.get('cast', [])[:5]]\n",
        "            if cast_list:\n",
        "                cast = ', '.join(cast_list) # 쉼표로 구분된 텍스트로 변환\n",
        "\n",
        "        # 찾은 감독과 배우 목록을 반환\n",
        "        return director, cast\n",
        "\n",
        "    except requests.exceptions.RequestException:\n",
        "        # 연결 오류 시 둘 다 에러로 처리\n",
        "        return \"Connection Error\", \"Connection Error\"\n",
        "\n",
        "def main():\n",
        "    \"\"\"메인 실행 함수\"\"\"\n",
        "    if API_KEY == 'YOUR_API_KEY':\n",
        "        print(\"\\n[오류] 코드의 API_KEY 부분을 본인의 TMDB API 키로 변경한 후 다시 실행해주세요.\")\n",
        "        return\n",
        "\n",
        "    print(\"TMDB API를 사용하여 감독 및 배우 정보를 가져오는 작업을 시작합니다.\")\n",
        "\n",
        "    try:\n",
        "        df = pd.read_csv('FILE_PATH', encoding='utf-8')\n",
        "        print(f\"\\n'filteredTMDB.csv' 파일을 성공적으로 읽었습니다. (총 {len(df)}개 행)\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"\\n[오류] 'filteredTMDB.csv' 파일을 찾을 수 없습니다.\")\n",
        "        return\n",
        "\n",
        "    # 각 영화 ID에 대해 get_credits 함수를 적용하고 결과를 저장할 리스트\n",
        "    results = []\n",
        "\n",
        "    # tqdm을 사용하여 진행 상황 표시\n",
        "    for movie_id in tqdm(df['id'], desc=\"감독/배우 정보 가져오는 중\"):\n",
        "        results.append(get_credits(movie_id))\n",
        "\n",
        "    # 결과를 새로운 컬럼으로 데이터프레임에 추가\n",
        "    df[['director', 'cast']] = results\n",
        "\n",
        "    # --- 결과 저장 ---\n",
        "    output_filename = 'TMDB_with_credits.csv'\n",
        "    df.to_csv(output_filename, index=False, encoding='utf-8-sig')\n",
        "\n",
        "    print(\"\\n--- 작업 완료 ---\")\n",
        "    print(f\"결과가 '{output_filename}' 파일에 저장되었습니다.\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**데이터 전처리**\n",
        "\n",
        "- 조건 1: 'backdrop_path', 'homepage', 'poster_path' 컬럼 삭제\n",
        "- 조건 2: 'vote_count'가 0인 행 삭제\n",
        "- 조건 3: 'vote_count'가 1이고 'keywords'에 'wrestling'이 포함된 행 삭제\n",
        "- 조건 4: 'genres', 'production_countries', 'director', 'keywords' 컬럼이 null인 행 삭제\n",
        "- 조건 5: Budget 30만 달러 미만 행 삭제\n",
        "- ROI 및 SR 계산 로직 추가\n",
        "- 흥행척도(y_result) 컬럼 추가"
      ],
      "metadata": {
        "id": "-aXER3EBh6zS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# 1. 파일 경로 설정\n",
        "# 코랩에 직접 파일을 업로드한 경우 이 경로를 사용합니다.\n",
        "file_path = '/content/TMDB_with_credits.csv'\n",
        "\n",
        "# 2. CSV 파일 불러오기\n",
        "try:\n",
        "    print(f\"Loading data from: {file_path}\")\n",
        "    df = pd.read_csv(file_path)\n",
        "    print(\"Data loaded successfully. Initial shape:\", df.shape)\n",
        "    print(\"\\nInitial DataFrame info:\")\n",
        "    df.info()\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file was not found at the specified path: {file_path}\")\n",
        "    print(\"Please make sure you have uploaded 'TMDB_with_credits.csv' to your Colab session.\")\n",
        "    # 파일이 없을 경우, 이후 코드 실행을 막기 위해 빈 데이터프레임 생성\n",
        "    df = pd.DataFrame()\n",
        "\n",
        "# 3. 데이터 가공 (파일이 성공적으로 로드된 경우에만 실행)\n",
        "if not df.empty:\n",
        "    # 복사본을 만들어 원본 데이터 보존\n",
        "    df_processed = df.copy()\n",
        "\n",
        "    # 조건 1: 'backdrop_path', 'homepage', 'poster_path' 컬럼 삭제\n",
        "    columns_to_drop = ['backdrop_path', 'homepage', 'poster_path']\n",
        "    # 해당 컬럼이 데이터프레임에 존재하는지 확인 후 삭제\n",
        "    existing_columns_to_drop = [col for col in columns_to_drop if col in df_processed.columns]\n",
        "    if existing_columns_to_drop:\n",
        "        df_processed.drop(columns=existing_columns_to_drop, inplace=True)\n",
        "        print(f\"\\nDropped columns: {existing_columns_to_drop}. Shape after dropping columns:\", df_processed.shape)\n",
        "    else:\n",
        "        print(\"\\nColumns to drop not found in the DataFrame.\")\n",
        "\n",
        "\n",
        "    # 조건 2: 'vote_count'가 0인 행 삭제\n",
        "    initial_rows = df_processed.shape[0]\n",
        "    df_processed = df_processed[df_processed['vote_count'] != 0]\n",
        "    print(f\"\\nRemoved rows with vote_count = 0. Shape after filtering:\", df_processed.shape)\n",
        "    print(f\"{initial_rows - df_processed.shape[0]} rows were removed.\")\n",
        "\n",
        "    # 조건 3: 'vote_count'가 1이고 'keywords'에 'wrestling'이 포함된 행 삭제\n",
        "    initial_rows = df_processed.shape[0]\n",
        "    # 'keywords' 컬럼이 문자열이 아닌 경우(NaN 등) 오류가 발생하지 않도록 astype(str)으로 변환하고,\n",
        "    # 'wrestling' 문자열을 포함하는지 확인합니다. na=False는 NaN 값을 False로 처리합니다.\n",
        "    condition_to_remove = (df_processed['vote_count'] == 1) & (df_processed['keywords'].astype(str).str.contains('wrestling', na=False))\n",
        "    # 위 조건에 해당하지 않는(~) 행들만 선택하여 데이터프레임을 갱신합니다.\n",
        "    df_processed = df_processed[~condition_to_remove]\n",
        "    print(f\"\\nRemoved rows with vote_count = 1 and 'wrestling' in keywords. Shape after filtering:\", df_processed.shape)\n",
        "    print(f\"{initial_rows - df_processed.shape[0]} rows were removed.\")\n",
        "\n",
        "    # 조건 4: 'genres', 'production_countries', 'director', 'keywords' 컬럼이 null인 행 삭제\n",
        "    columns_to_check_null = ['genres', 'production_countries', 'director', 'keywords']\n",
        "    initial_rows = df_processed.shape[0]\n",
        "    # 해당 컬럼들이 존재하는지 확인\n",
        "    existing_columns_to_check = [col for col in columns_to_check_null if col in df_processed.columns]\n",
        "    if existing_columns_to_check:\n",
        "        df_processed.dropna(subset=existing_columns_to_check, inplace=True)\n",
        "        print(f\"\\nRemoved rows with null values in {existing_columns_to_check}. Final shape:\", df_processed.shape)\n",
        "        print(f\"{initial_rows - df_processed.shape[0]} rows were removed.\")\n",
        "    else:\n",
        "        print(\"\\nColumns to check for nulls not found in the DataFrame.\")\n",
        "\n",
        "    # 조건 5: Budget 30만 달러 미만 행 삭제\n",
        "    df_processed = df_processed[df_processed['budget'] >= 300000]\n",
        "\n",
        "    # --- ROI 및 SR 계산 로직 추가 ---\n",
        "\n",
        "    # 1. ROI(수익률) 계산 컬럼 추가\n",
        "    df_processed['ROI'] = ((df_processed['revenue'] - df_processed['budget']) / df_processed['budget']) * 100\n",
        "\n",
        "    # 2. SR(흥행 등급) 계산 컬럼 추가\n",
        "    conditions = [\n",
        "        (df_processed['revenue'] / df_processed['budget']) >= 3,\n",
        "        (df_processed['revenue'] / df_processed['budget']) >= 2.5,\n",
        "        (df_processed['revenue'] / df_processed['budget']) >= 2,\n",
        "    ]\n",
        "    choices = [3, 2, 1]\n",
        "    df_processed['SR'] = np.select(conditions, choices, default=0)\n",
        "\n",
        "    # 3. y_result 컬럼 추가\n",
        "    df_processed['y_result'] = 0\n",
        "\n",
        "    # ROI 조건\n",
        "    df_processed['y_result'] = df_processed['y_result'] + np.where(df_processed['ROI'] > 2.5, 1, 0)\n",
        "\n",
        "    # SR 조건\n",
        "    df_processed['y_result'] = df_processed['y_result'] + np.where(df_processed['SR'] > 2, 1, 0)\n",
        "\n",
        "    # 예산 및 매출 조건\n",
        "    commercial_cond = df_processed['budget'] <= 100000000\n",
        "    blockbuster_cond = df_processed['budget'] > 100000000\n",
        "\n",
        "    df_processed['y_result'] = df_processed['y_result'] + np.where(\n",
        "        commercial_cond & (df_processed['revenue'] >= 100000000), 1, 0)\n",
        "    df_processed['y_result'] = df_processed['y_result'] + np.where(\n",
        "        blockbuster_cond & (df_processed['revenue'] >= 500000000), 1, 0)\n",
        "\n",
        "\n",
        "    # 4. 가공된 데이터 확인\n",
        "    print(\"\\n--- Processed Data Sample ---\")\n",
        "    print(df_processed.head())\n",
        "\n",
        "    # 5. 가공된 파일을 새로운 CSV로 저장\n",
        "    # 원본 파일이 있던 디렉토리에 저장합니다.\n",
        "    output_directory = os.path.dirname(file_path) if os.path.dirname(file_path) else '/content/'\n",
        "    output_filename = 'TMDB_processed_final.csv'\n",
        "    output_path = os.path.join(output_directory, output_filename)\n",
        "\n",
        "    try:\n",
        "        df_processed.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
        "        print(f\"\\nProcessed data successfully saved to: {output_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nAn error occurred while saving the file: {e}\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nSkipping data processing due to file loading error.\")"
      ],
      "metadata": {
        "id": "dME80plEh_A8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 가설 1\n",
        "원작, 실화 기반 영화의 흥행력이 높을 것이다."
      ],
      "metadata": {
        "id": "jGQxzaJrWaxz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 데이터 불러오기\n",
        "df = pd.read_csv('./TMDB_processed_final.csv')\n",
        "keyword_columns = ['keywords']\n",
        "\n",
        "# 'based on' 포함 여부를 나타내는 마스크 생성\n",
        "mask = df[keyword_columns].astype(str).apply(lambda x: x.str.contains('based on', case=False, na=False)).any(axis=1)\n",
        "\n",
        "# 'is_based_on_original' 열 추가\n",
        "# 원작 기반이면 1, 아니면 0으로 구분합니다.\n",
        "df['is_based_on_original'] = np.where(mask, 1, 0)\n",
        "\n",
        "# 두 그룹의 영화 수 확인\n",
        "print(df['is_based_on_original'].value_counts())\n",
        "\n",
        "# 원작 기반 여부와 흥행 결과(y_result) 간의 교차표 생성\n",
        "contingency_table = pd.crosstab(df['is_based_on_original'], df['y_result'])\n",
        "\n",
        "print(\"--- 교차표 (빈도) ---\")\n",
        "print(contingency_table)\n",
        "\n",
        "contingency_table_ratio = pd.crosstab(df['is_based_on_original'], df['y_result'], normalize='index')\n",
        "\n",
        "print(\"\\n--- 교차표 (그룹 내 비율) ---\")\n",
        "print(contingency_table_ratio)\n",
        "\n",
        "# 시각화하여 확인\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "contingency_table_ratio.plot(kind='bar', stacked=True)\n",
        "plt.title('Proportion of Box Office Success by Movie Type')\n",
        "plt.xlabel('Movie Type (0: Original, 1: Based on Original)')\n",
        "plt.ylabel('Proportion')\n",
        "plt.xticks(rotation=0)\n",
        "plt.legend(title='y_result')\n",
        "plt.savefig('success_proportion_by_type.png')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "## 카이제곱 검정\n",
        "\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# 2단계에서 만든 교차표를 카이제곱 검정에 사용\n",
        "chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n",
        "\n",
        "print(f\"카이제곱 통계량: {chi2:.4f}\")\n",
        "print(f\"P-value: {p_value}\")\n",
        "\n",
        "# p-value를 기준으로 가설을 판단합니다.\n",
        "alpha = 0.05  # 유의수준 5%\n",
        "if p_value < alpha:\n",
        "    print(f\"\\nP-value ({p_value:.4f})가 유의수준 ({alpha})보다 작으므로, 귀무가설을 기각합니다.\")\n",
        "    print(\">> 결론: 원작 기반 여부와 흥행 결과는 통계적으로 유의미한 관련이 있습니다.\")\n",
        "else:\n",
        "    print(f\"\\nP-value ({p_value:.4f})가 유의수준 ({alpha})보다 크므로, 귀무가설을 기각할 수 없습니다.\")\n",
        "    print(\">> 결론: 원작 기반 여부와 흥행 결과가 관련이 있다는 통계적 근거를 찾지 못했습니다.\")"
      ],
      "metadata": {
        "id": "_DgM2yrOiFCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 가설 2\n",
        "과거에 흥행한 이력이 있는 감독은 차기작도 흥행할 것이다."
      ],
      "metadata": {
        "id": "Cf3AQcX4Wkdl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## 추가 전처리\n",
        "\n",
        "# 연도순 정렬 (release_date 컬럼이 날짜형인지 확인 후 변환)\n",
        "df1['release_date'] = pd.to_datetime(df1['release_date'])\n",
        "df1 = df1.sort_values(by=['director', 'release_date']).reset_index(drop=True)\n",
        "\n",
        "# 첫 번째 칼럼: 감독이 이전에 찍은 영화의 개수\n",
        "df1['di_count'] = (\n",
        "    df1.groupby('director')\n",
        "       .cumcount()\n",
        ")\n",
        "\n",
        "# 두 번째 칼럼: 감독이 이전에 찍은 영화 중 성공(y_result>=2) 개수\n",
        "df1['sucessed'] = (\n",
        "    df1.groupby('director')['y_result']\n",
        "       .apply(lambda x: (x >= 2).cumsum().shift(fill_value=0))\n",
        "       .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "# 세 번째 칼럼: ratio = sucessed / di_count (0 division 방지)\n",
        "df1['ratio'] = df1.apply(\n",
        "    lambda row: row['sucessed'] / row['di_count'] if row['di_count'] > 0 else 0,\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "df1.tail(200)\n",
        "\n",
        "df1.to_csv(\"tmdb_profinal_di_counts.csv\", index=False)\n",
        "\n",
        "\n",
        "## t-test\n",
        "\n",
        "# ratio 기준으로 두 그룹 나누기 (중위수/평균/임계치 기준 선택 가능)\n",
        "threshold = df1['ratio'].median()  # 여기서는 median(중위수) 기준으로 나눔\n",
        "\n",
        "group_high = df1[df1['ratio'] > threshold]['y_result']\n",
        "group_low = df1[df1['ratio'] <= threshold]['y_result']\n",
        "\n",
        "# 독립표본 t-test\n",
        "t_stat, p_value = stats.ttest_ind(group_high, group_low, equal_var=False)  # Welch’s t-test\n",
        "\n",
        "print(\"고 ratio 그룹(양수) y_result 평균:\", group_high.mean())\n",
        "print(\"저 ratio 그룹(음수) y_result 평균:\", group_low.mean())\n",
        "print(\"t-통계량:\", t_stat)\n",
        "print(\"p-값:\", p_value)\n",
        "\n",
        "if p_value < 0.05:\n",
        "    print(\"✅ 유의수준 0.05에서 두 그룹 간 차이가 통계적으로 유의합니다.\")\n",
        "else:\n",
        "    print(\"❌ 유의한 차이를 찾지 못했습니다.\")\n",
        "\n",
        "\n",
        "## 아노바 검정\n",
        "\n",
        "# df1이 있다고 가정\n",
        "df = df1.copy()\n",
        "\n",
        "# 세 구간 정의\n",
        "group1 = df[(df['ratio'] >= 0.0) & (df['ratio'] < 0.2)]['y_result']   # [0,0.2)\n",
        "group2 = df[(df['ratio'] >= 0.2) & (df['ratio'] < 0.7)]['y_result']   # [0.2,0.7)\n",
        "group3 = df[(df['ratio'] >= 0.7) & (df['ratio'] < 1.0)]['y_result']  # [0.7,1]\n",
        "\n",
        "# 샘플 수 확인\n",
        "print(\"샘플 수:\")\n",
        "print(\"[0,0.2):\", len(group1))\n",
        "print(\"[0.2,0.7):\", len(group2))\n",
        "print(\"[0.7,1]:\", len(group3))\n",
        "print()\n",
        "\n",
        "# 일원분산분석 (One-way ANOVA)\n",
        "f_stat, p_value = stats.f_oneway(group1, group2, group3)\n",
        "\n",
        "print(\"ANOVA F-통계량:\", f_stat)\n",
        "print(\"ANOVA p-value :\", p_value)\n",
        "\n",
        "# 각 그룹 평균 출력\n",
        "print(\"\\n그룹 평균 y_result: ratio 구간이 높아질수록, 즉 감독의 전작 성공률이 높아질수 평균 y_result 상승\")\n",
        "print(\"[0,0.2):\", group1.mean())\n",
        "print(\"[0.2,0.7):\", group2.mean())\n",
        "print(\"[0.7,1]:\", group3.mean())\n",
        "\n",
        "\n",
        "## 시각화\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# 데이터프레임 df 에서 그룹 나누기\n",
        "group1 = df1[(df1['ratio'] >= 0.0) & (df1['ratio'] < 0.2)]['y_result']\n",
        "group2 = df1[(df1['ratio'] >= 0.2) & (df1['ratio'] < 0.7)]['y_result']\n",
        "group3 = df1[(df1['ratio'] >= 0.7) & (df1['ratio'] < 1.0)]['y_result']\n",
        "\n",
        "# 시각화를 위해 데이터 묶기\n",
        "plot_df = pd.DataFrame({\n",
        "    \"y_result\": pd.concat([group1, group2, group3], ignore_index=True),\n",
        "    \"Group\": ([\"[0,0.2)\"] * len(group1)) +\n",
        "             ([\"[0.2,0.7)\"] * len(group2)) +\n",
        "             ([\"[0.7,1)\"] * len(group3))\n",
        "})\n",
        "\n",
        "# 박스플롯\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.boxplot(x=\"Group\", y=\"y_result\", data=plot_df, palette=\"Set2\")\n",
        "sns.pointplot(x=\"Group\", y=\"y_result\", data=plot_df, estimator=\"mean\",\n",
        "              color=\"red\", markers=\"D\", linestyles=\"\")\n",
        "plt.title(\"그룹별 y_result 분포 (박스플롯)\")\n",
        "plt.show()\n",
        "\n",
        "# 바플롯 (평균 + 신뢰구간)\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.barplot(x=\"Group\", y=\"y_result\", data=plot_df, palette=\"Set2\", ci=95)\n",
        "plt.title(\"그룹별 평균 y_result (Barplot + 95% CI)\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "## 사후검정 TUKEY HSD\n",
        "\n",
        "df = df1.copy()\n",
        "group1 = df[(df['ratio'] >= 0.0) & (df['ratio'] < 0.2)]['y_result']   # [0,0.2)\n",
        "group2 = df[(df['ratio'] >= 0.2) & (df['ratio'] < 0.7)]['y_result']   # [0.2,0.7)\n",
        "group3 = df[(df['ratio'] >= 0.7) & (df['ratio'] < 1.0)]['y_result']   # [0.7,1)\n",
        "\n",
        "# --- (선택) 등분산성 체크: 등분산이 아니면 Tukey 대신 Games-Howell 권장 ---\n",
        "lev_stat, lev_p = stats.levene(group1, group2, group3, center='median')\n",
        "print(f\"Levene 등분산성 p-value: {lev_p:.4g}\")\n",
        "\n",
        "# --- Tukey HSD 실행 ---\n",
        "# 시각화/검정을 위해 하나의 시리즈로 묶고 그룹 라벨을 부여\n",
        "y = pd.concat([group1, group2, group3], ignore_index=True)\n",
        "g = ([\"[0,0.2)\"] * len(group1) +\n",
        "     [\"[0.2,0.7)\"] * len(group2) +\n",
        "     [\"[0.7,1)\"] * len(group3))\n",
        "\n",
        "tukey = pairwise_tukeyhsd(endog=y, groups=g, alpha=0.05)\n",
        "\n",
        "print(\"\\n[Tukey HSD 결과]\")\n",
        "print(tukey)  # 표 형태로 요약 출력\n",
        "\n",
        "# 결과를 DataFrame으로 활용하고 싶다면\n",
        "tukey_df = pd.DataFrame(\n",
        "    tukey._results_table.data[1:],  # 헤더 제외\n",
        "    columns=tukey._results_table.data[0]\n",
        ")\n",
        "print(\"\\nDataFrame 형태 요약:\")\n",
        "print(tukey_df)"
      ],
      "metadata": {
        "id": "9GLjabwwWpZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 가설 3\n",
        "시리즈물은 1편보다 후속작의 흥행 확률이 더 높을 것이다."
      ],
      "metadata": {
        "id": "MLpUtNdIWqBm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 시리즈물 뽑아내기\n",
        "\n",
        "# ===== 시리즈물 추출 & 그룹핑 (하이브리드 규칙 + 쉼표 split 적용) =====\n",
        "# 전제: df = pd.read_csv(\"TMDB_processed_final.csv\") 완료됨\n",
        "# 필요 컬럼: ['title','release_date','director','cast','keywords']\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from collections import defaultdict\n",
        "from itertools import combinations\n",
        "\n",
        "df = pd.read_csv('/content/TMDB_processed_final.csv')\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 1) 유틸 함수\n",
        "# -----------------------------\n",
        "STOPWORDS = set(\"\"\"a an and the of to in on at for from with by or as into about over under after before\n",
        "part episode chapter pt vol volume saga story legend rise return revenge war age dawn day night vs versus\n",
        "don t s a an the of and\"\"\".split())\n",
        "ROMAN = {\"i\",\"ii\",\"iii\",\"iv\",\"v\",\"vi\",\"vii\",\"viii\",\"ix\",\"x\"}\n",
        "\n",
        "def normalize_title(t: str) -> str:\n",
        "    if not isinstance(t, str):\n",
        "        return \"\"\n",
        "    t = t.lower().strip()\n",
        "    t = re.sub(r\"\\s*\\([^()]*\\)\\s*$\", \"\", t)           # 끝 괄호 부제 제거\n",
        "    t = re.sub(r\"[^\\w\\s:\\-]\", \" \", t)                 # 문장부호 정리(콜론/대시는 유지)\n",
        "    t = re.sub(r\"\\s{2,}\", \" \", t).strip()\n",
        "    return t\n",
        "\n",
        "def tokenize(t: str):\n",
        "    return [w for w in re.split(r\"[\\s:\\-]+\", t) if w]\n",
        "\n",
        "def remove_leading_numbers(tokens):\n",
        "    i = 0\n",
        "    while i < len(tokens) and (tokens[i].isdigit() or tokens[i] in ROMAN):\n",
        "        i += 1\n",
        "    return tokens[i:]\n",
        "\n",
        "def sig_tokens(tokens, max_k=None):\n",
        "    sig = [w for w in tokens if (w not in STOPWORDS and not w.isdigit() and w not in ROMAN)]\n",
        "    return sig[:max_k] if max_k else sig\n",
        "\n",
        "def anchor2(title):\n",
        "    \"\"\"제목 전체에서 유의미한 앞 2토큰\"\"\"\n",
        "    t = normalize_title(title)\n",
        "    toks = remove_leading_numbers(tokenize(t))\n",
        "    sig = sig_tokens(toks)\n",
        "    return \" \".join(sig[:2]) if len(sig) >= 2 else \"\"\n",
        "\n",
        "def base3(title):\n",
        "    \"\"\"콜론/대시 앞의 첫 세그먼트에서 유의미한 앞 3토큰\"\"\"\n",
        "    t = normalize_title(title)\n",
        "    first_seg = re.split(r\"[:\\-]\", t, maxsplit=1)[0].strip()\n",
        "    toks = remove_leading_numbers(tokenize(first_seg))\n",
        "    sig = sig_tokens(toks)\n",
        "    return \" \".join(sig[:3]) if sig else \"\"\n",
        "\n",
        "def has_seq_suffix(title):\n",
        "    \"\"\"제목 끝의 숫자/로마숫자/Part/Chapter/Episode/Vol 패턴\"\"\"\n",
        "    t = normalize_title(title)\n",
        "    if re.search(r\"(part|chapter|episode|pt|vol|volume)\\s*(\\d+|i|ii|iii|iv|v|vi|vii|viii|ix|x)\\s*$\", t):\n",
        "        return True\n",
        "    if re.search(r\"\\b(\\d+|i|ii|iii|iv|v|vi|vii|viii|ix|x)\\s*$\", t):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def year_from_date(s):\n",
        "    try:\n",
        "        return int(str(s)[:4])\n",
        "    except:\n",
        "        return np.nan\n",
        "\n",
        "# === 여기서 배우/키워드 split ===\n",
        "def parse_csv_list(s, max_n=None, lower=True):\n",
        "    \"\"\"쉼표 기준 split → strip → lower → 리스트\"\"\"\n",
        "    if not isinstance(s, str):\n",
        "        return []\n",
        "    items = [x.strip() for x in s.split(\",\") if x.strip()]\n",
        "    if lower:\n",
        "        items = [x.lower() for x in items]\n",
        "    return items[:max_n] if max_n else items\n",
        "\n",
        "def jacc(a, b):\n",
        "    A, B = set(a), set(b)\n",
        "    if not A or not B:\n",
        "        return 0.0\n",
        "    return len(A & B) / len(A | B)\n",
        "\n",
        "# -----------------------------\n",
        "# 2) 특징 생성\n",
        "# -----------------------------\n",
        "df = df.copy()\n",
        "df['year'] = df['release_date'].apply(year_from_date)\n",
        "df['anchor2'] = df['title'].fillna(\"\").apply(anchor2)\n",
        "df['base3']   = df['title'].fillna(\"\").apply(base3)\n",
        "df['seq_suffix'] = df['title'].fillna(\"\").apply(has_seq_suffix)\n",
        "\n",
        "# 배우: 상위 5명, lead3: 상위 3명\n",
        "df['cast_list'] = df['cast'].apply(lambda s: parse_csv_list(s, max_n=5))\n",
        "df['lead3']     = df['cast'].apply(lambda s: parse_csv_list(s, max_n=3))\n",
        "# 키워드: 전체 사용\n",
        "df['keywords_list'] = df['keywords'].apply(lambda s: parse_csv_list(s))\n",
        "\n",
        "df.replace({\"anchor2\": {\"\": np.nan}, \"base3\": {\"\": np.nan}}, inplace=True)\n",
        "\n",
        "# -----------------------------\n",
        "# 3) 같은 anchor2 내에서 점수 계산\n",
        "# -----------------------------\n",
        "groups = defaultdict(list)\n",
        "for idx, row in df.dropna(subset=['anchor2']).iterrows():\n",
        "    groups[row['anchor2']].append(idx)\n",
        "\n",
        "def qualify_pair(i, j):\n",
        "    a = df.loc[i]; b = df.loc[j]\n",
        "    if a['anchor2'] != b['anchor2']:\n",
        "        return False, 0.0\n",
        "\n",
        "    base_match   = (a['base3'] == b['base3'])\n",
        "    same_director = isinstance(a['director'], str) and a['director'] != \"\" and a['director'] == b['director']\n",
        "    cast_sim     = jacc(a['cast_list'], b['cast_list'])\n",
        "    lead_overlap = len(set(a['lead3']) & set(b['lead3'])) >= 1\n",
        "    keyw_sim     = jacc(a['keywords_list'], b['keywords_list'])\n",
        "    ygap         = abs((a['year'] if pd.notna(a['year']) else 0) - (b['year'] if pd.notna(b['year']) else 0))\n",
        "    has_sequel   = a['seq_suffix'] or b['seq_suffix']\n",
        "\n",
        "    # 점수화\n",
        "    score = 0.0\n",
        "    if has_sequel:       score += 0.50\n",
        "    if same_director:    score += 0.30\n",
        "    if cast_sim >= 0.25: score += 0.25\n",
        "    if lead_overlap:     score += 0.25\n",
        "    if keyw_sim >= 0.50: score += 0.15\n",
        "    if pd.notna(a['year']) and pd.notna(b['year']) and ygap <= 12: score += 0.10\n",
        "\n",
        "    threshold = 0.80 if base_match else 0.95\n",
        "    return score >= threshold, score\n",
        "\n",
        "edges = []\n",
        "for key, idxs in groups.items():\n",
        "    if len(idxs) < 2:\n",
        "        continue\n",
        "    for i, j in combinations(idxs, 2):\n",
        "        ok, _ = qualify_pair(i, j)\n",
        "        if ok:\n",
        "            edges.append((i, j))\n",
        "\n",
        "# -----------------------------\n",
        "# 4) Union-Find로 그룹핑\n",
        "# -----------------------------\n",
        "parent = {}\n",
        "def find(x):\n",
        "    parent.setdefault(x, x)\n",
        "    if parent[x] != x:\n",
        "        parent[x] = find(parent[x])\n",
        "    return parent[x]\n",
        "def union(a, b):\n",
        "    ra, rb = find(a), find(b)\n",
        "    if ra != rb:\n",
        "        parent[rb] = ra\n",
        "\n",
        "for i, j in edges:\n",
        "    union(i, j)\n",
        "\n",
        "comp = defaultdict(list)\n",
        "for idx in set([k for e in edges for k in e]):\n",
        "    comp[find(idx)].append(idx)\n",
        "\n",
        "# -----------------------------\n",
        "# 5) 결과 DataFrame\n",
        "# -----------------------------\n",
        "rows = []\n",
        "for r, idxs in comp.items():\n",
        "    titles = df.loc[idxs, 'title'].tolist()\n",
        "    years  = df.loc[idxs, 'year'].dropna().astype(int).tolist()\n",
        "    name   = df.loc[idxs, 'anchor2'].value_counts().idxmax()\n",
        "    rows.append({\n",
        "        \"series_key\": r,\n",
        "        \"series_name\": name,\n",
        "        \"n_titles\": len(idxs),\n",
        "        \"years_min\": min(years) if years else np.nan,\n",
        "        \"years_max\": max(years) if years else np.nan,\n",
        "        \"titles\": sorted(titles)\n",
        "    })\n",
        "\n",
        "series_df = pd.DataFrame(rows).sort_values([\"n_titles\",\"series_name\"], ascending=[False, True]).reset_index(drop=True)\n",
        "\n",
        "# 원본 df에 라벨 부여\n",
        "in_series_titles = set(t for r in rows for t in r[\"titles\"])\n",
        "df['is_series']   = df['title'].isin(in_series_titles)\n",
        "name_map = {}\n",
        "for r in rows:\n",
        "    for t in r[\"titles\"]:\n",
        "        name_map[t] = r[\"series_name\"]\n",
        "df['series_group'] = df['title'].map(name_map)\n",
        "\n",
        "# -----------------------------\n",
        "# 6) 사용 예\n",
        "# -----------------------------\n",
        "print(\"감지된 시리즈 그룹 수:\", len(series_df))\n",
        "print(series_df.head(15)[[\"series_name\",\"n_titles\",\"years_min\",\"years_max\"]])\n",
        "\n",
        "# series_df.to_csv(\"series_groups_final.csv\", index=False)\n",
        "# df.to_csv(\"TMDB_with_series_labels.csv\", index=False)\n",
        "\n",
        "\n",
        "# 추론 통계 코드\n",
        "\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "TMDB_processed_final_with_series.csv 기반\n",
        "'계층적 비교(1→2, 2→3, …; 중간 없으면 1→3처럼 다음 유효 편과 비교)'에 대해\n",
        "부호검정(Sign test)과 윌콕슨 부호순위검정(Wilcoxon signed-rank)을 함께 수행하여\n",
        "방향(누가 더 우세?)과 차이의 크기(효과 크기, 중앙값/평균)를 모두 보여주는 스크립트.\n",
        "\n",
        "- y_result 컬럼은 CSV에 이미 존재한다고 가정 (0~3의 순서형 점수)\n",
        "- Avengers(1998)는 avengers 시리즈에서 제외\n",
        "- 결과 저장(현재 작업 폴더):\n",
        "    1) pairs_level_details.csv      : 모든 시리즈의 (i→j) 페어 상세 및 diff\n",
        "    2) level_tests_summary.csv      : 각 전이 레벨(예: 1→2, 2→3, 1→3 등)별 검정 요약\n",
        "    3) overall_tests_summary.csv    : 전체 페어에 대한 검정 요약\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "# SciPy가 있으면 정확 검정 사용, 없으면 대체 절차(정규근사/부호검정)로 동작\n",
        "try:\n",
        "    from scipy.stats import binomtest, wilcoxon\n",
        "    SCIPY = True\n",
        "except Exception:\n",
        "    SCIPY = False\n",
        "\n",
        "# ---------- (0) 파일 로드 ----------\n",
        "PATH = \"/content/TMDB_processed_final_with_series.csv\"\n",
        "assert os.path.exists(PATH), f\"파일이 없습니다: {PATH}\"\n",
        "df = pd.read_csv(PATH)\n",
        "\n",
        "# ---------- (1) 컬럼명 설정 (필요시 아래만 수정) ----------\n",
        "title_col = \"title\"\n",
        "series_col = \"series_group\"\n",
        "release_date_col = \"release_date\"\n",
        "yresult_col = \"y_result\"\n",
        "\n",
        "# ---------- (2) 연도 추출(정렬용) ----------\n",
        "df[\"year_extracted\"] = pd.to_datetime(df[release_date_col], errors=\"coerce\").dt.year\n",
        "\n",
        "# ---------- (3) Avengers(1998) 시리즈 제외 ----------\n",
        "mask_avengers_1998 = (\n",
        "    df[title_col].astype(str).str.strip().str.lower().eq(\"the avengers\")\n",
        "    & (df[\"year_extracted\"] == 1998)\n",
        "    & df[series_col].astype(str).str.lower().str.contains(\"avengers\", na=False)\n",
        ")\n",
        "df.loc[mask_avengers_1998, series_col] = np.nan\n",
        "print(f\"[INFO] 'The Avengers (1998)' 시리즈 그룹 제외 행 수: {int(mask_avengers_1998.sum())}\")\n",
        "\n",
        "# ---------- (4) 시리즈 내 정렬 및 installment 부여 ----------\n",
        "series_df = df[df[series_col].notna()].copy()\n",
        "series_df = series_df.sort_values(\n",
        "    by=[series_col, \"year_extracted\", release_date_col, title_col],\n",
        "    ascending=[True, True, True, True],\n",
        ")\n",
        "series_df[\"installment\"] = series_df.groupby(series_col).cumcount() + 1\n",
        "series_df[yresult_col] = pd.to_numeric(series_df[yresult_col], errors=\"coerce\")\n",
        "\n",
        "# ---------- (5) '가장 가까운 다음 편'과 짝짓기\n",
        "# 정렬된 순서에서 인접 행끼리 묶기 → 중간 편이 비어 있으면 자연스럽게 1→3 같은 페어가 만들어짐\n",
        "pairs = []\n",
        "for grp, g in series_df.groupby(series_col, sort=False):\n",
        "    g = g.sort_values(\"installment\")\n",
        "    rows = list(g.itertuples(index=False))\n",
        "    for a, b in zip(rows[:-1], rows[1:]):   # 인접 행끼리 페어링\n",
        "        i_inst = int(getattr(a, \"installment\"))\n",
        "        j_inst = int(getattr(b, \"installment\"))\n",
        "        i_y = getattr(a, yresult_col)\n",
        "        j_y = getattr(b, yresult_col)\n",
        "        pairs.append({\n",
        "            \"series_group\": grp,\n",
        "            \"i_installment\": i_inst,\n",
        "            \"j_installment\": j_inst,\n",
        "            \"level_label\": f\"{i_inst}->{j_inst}\",  # 계층적 전이 라벨\n",
        "            \"i_title\": getattr(a, title_col),\n",
        "            \"j_title\": getattr(b, title_col),\n",
        "            \"i_year\": int(getattr(a, \"year_extracted\")) if pd.notna(getattr(a, \"year_extracted\")) else None,\n",
        "            \"j_year\": int(getattr(b, \"year_extracted\")) if pd.notna(getattr(b, \"year_extracted\")) else None,\n",
        "            \"i_y_result\": i_y,\n",
        "            \"j_y_result\": j_y,\n",
        "        })\n",
        "\n",
        "pairs_df = pd.DataFrame(pairs).dropna(subset=[\"i_y_result\", \"j_y_result\"])\n",
        "pairs_df[\"i_y_result\"] = pairs_df[\"i_y_result\"].astype(int)\n",
        "pairs_df[\"j_y_result\"] = pairs_df[\"j_y_result\"].astype(int)\n",
        "pairs_df[\"diff\"] = pairs_df[\"j_y_result\"] - pairs_df[\"i_y_result\"]\n",
        "pairs_df[\"win\"] = np.where(pairs_df[\"diff\"] > 0, 1, np.where(pairs_df[\"diff\"] < 0, -1, 0))\n",
        "\n",
        "print(f\"[INFO] 생성된 계층 페어 수: {len(pairs_df)}\")\n",
        "if len(pairs_df) == 0:\n",
        "    raise SystemExit(\"[ERROR] 유효한 페어가 없습니다. 시리즈/컬럼 구성을 확인해 주세요.\")\n",
        "\n",
        "# ---------- (6) 검정 함수 ----------\n",
        "def sign_test(win_series):\n",
        "    \"\"\"부호검정(Sign test)을 이항검정으로 수행: H1: 후속 승 비율 > 0.5\"\"\"\n",
        "    s = win_series.dropna()\n",
        "    s = s[s != 0]  # 동률/NA 제외\n",
        "    n_plus = int((s == 1).sum())\n",
        "    n_minus = int((s == -1).sum())\n",
        "    n = n_plus + n_minus\n",
        "    if n == 0:\n",
        "        return {\"n_eff\": 0, \"wins_sequel\": n_plus, \"wins_prior\": n_minus, \"p_one\": np.nan, \"p_two\": np.nan}\n",
        "    if SCIPY:\n",
        "        p_two = binomtest(n_plus, n, 0.5, alternative=\"two-sided\").pvalue\n",
        "        p_one = binomtest(n_plus, n, 0.5, alternative=\"greater\").pvalue\n",
        "    else:\n",
        "        # 정규근사(간이)\n",
        "        phat = n_plus / n\n",
        "        se = math.sqrt(0.25 / n)\n",
        "        z = (phat - 0.5) / se\n",
        "        p_one = 0.5 * (1 - math.erf(z / math.sqrt(2)))\n",
        "        p_two = min(1.0, 2 * p_one)\n",
        "    return {\"n_eff\": n, \"wins_sequel\": n_plus, \"wins_prior\": n_minus, \"p_one\": p_one, \"p_two\": p_two}\n",
        "\n",
        "def wilcoxon_or_sign(diff_series):\n",
        "    \"\"\"Wilcoxon 한쪽/양쪽 p; SciPy 없으면 부호검정 대체\"\"\"\n",
        "    d = diff_series.dropna()\n",
        "    d = d[d != 0]\n",
        "    if len(d) == 0:\n",
        "        return {\"n_eff\": 0, \"stat\": np.nan, \"p_one\": np.nan, \"p_two\": np.nan}\n",
        "    if SCIPY:\n",
        "        stat_g, p_one = wilcoxon(d, alternative=\"greater\", zero_method=\"wilcox\")\n",
        "        stat_t, p_two = wilcoxon(d, alternative=\"two-sided\", zero_method=\"wilcox\")\n",
        "        return {\"n_eff\": int(len(d)), \"stat\": float(stat_g), \"p_one\": float(p_one), \"p_two\": float(p_two)}\n",
        "    else:\n",
        "        # 부호검정으로 대체\n",
        "        n_plus = int((d > 0).sum()); n_minus = int((d < 0).sum()); n = n_plus + n_minus\n",
        "        if n == 0:\n",
        "            return {\"n_eff\": 0, \"stat\": np.nan, \"p_one\": np.nan, \"p_two\": np.nan}\n",
        "        phat = n_plus / n\n",
        "        se = math.sqrt(0.25 / n)\n",
        "        z = (phat - 0.5) / se\n",
        "        p_one = 0.5 * (1 - math.erf(z / math.sqrt(2)))\n",
        "        p_two = min(1.0, 2 * p_one)\n",
        "        return {\"n_eff\": n, \"stat\": np.nan, \"p_one\": p_one, \"p_two\": p_two}\n",
        "\n",
        "def summarize_block(block_df, label=\"OVERALL\"):\n",
        "    \"\"\"한 전이 레벨(예: 1→2) 또는 전체에 대해 Sign/Wilcoxon + 효과크기 요약\"\"\"\n",
        "    wins = block_df[\"win\"].fillna(0)\n",
        "    sign_res = sign_test(wins)\n",
        "\n",
        "    wres = wilcoxon_or_sign(block_df[\"diff\"])\n",
        "\n",
        "    # 효과 크기(차이의 크기) 요약: 중앙값/평균/표준편차, 양/음/0 개수\n",
        "    diff = block_df[\"diff\"]\n",
        "    diff_nz = diff[diff != 0].dropna()\n",
        "    summary = {\n",
        "        \"level_label\": label,\n",
        "        \"pairs_total\": int(len(block_df)),\n",
        "        \"pairs_used_sign\": int(sign_res[\"n_eff\"]),\n",
        "        \"pairs_used_wilcoxon\": int(wres[\"n_eff\"]),\n",
        "        \"wins_sequel\": int(sign_res[\"wins_sequel\"]),\n",
        "        \"wins_prior\": int(sign_res[\"wins_prior\"]),\n",
        "        \"ties_or_na\": int((wins == 0).sum()),\n",
        "        \"diff_median\": float(np.nanmedian(diff)) if len(diff) > 0 else np.nan,\n",
        "        \"diff_mean\": float(np.nanmean(diff)) if len(diff) > 0 else np.nan,\n",
        "        \"diff_std\": float(np.nanstd(diff, ddof=1)) if len(diff) > 1 else np.nan,\n",
        "        \"diff_pos_count\": int((diff > 0).sum()),\n",
        "        \"diff_neg_count\": int((diff < 0).sum()),\n",
        "        \"diff_zero_count\": int((diff == 0).sum()),\n",
        "        \"sign_p_one_sided(H1: sequel > prior)\": sign_res[\"p_one\"],\n",
        "        \"sign_p_two_sided\": sign_res[\"p_two\"],\n",
        "        \"wilcoxon_stat\": wres[\"stat\"],\n",
        "        \"wilcoxon_p_one_sided(H1: sequel > prior)\": wres[\"p_one\"],\n",
        "        \"wilcoxon_p_two_sided\": wres[\"p_two\"],\n",
        "    }\n",
        "    return summary\n",
        "\n",
        "# ---------- (7) 레벨별 요약 ----------\n",
        "level_summaries = []\n",
        "for lvl, g in pairs_df.groupby(\"level_label\", sort=False):\n",
        "    level_summaries.append(summarize_block(g, label=lvl))\n",
        "\n",
        "level_summary_df = pd.DataFrame(level_summaries).sort_values(\n",
        "    by=[\"level_label\", \"pairs_total\"], ascending=[True, False]\n",
        ")\n",
        "\n",
        "# ---------- (8) 전체(OVERALL) 요약 ----------\n",
        "overall_summary_df = pd.DataFrame([summarize_block(pairs_df, label=\"OVERALL\")])\n",
        "\n",
        "# ---------- (9) 저장 ----------\n",
        "pairs_df.to_csv(\"pairs_level_details.csv\", index=False)\n",
        "level_summary_df.to_csv(\"level_tests_summary.csv\", index=False)\n",
        "overall_summary_df.to_csv(\"overall_tests_summary.csv\", index=False)\n",
        "\n",
        "print(\"[저장 완료] pairs_level_details.csv\")\n",
        "print(\"[저장 완료] level_tests_summary.csv\")\n",
        "print(\"[저장 완료] overall_tests_summary.csv\")\n",
        "\n",
        "# ---------- (10) 콘솔 표시(요약) ----------\n",
        "print(\"\\n=== OVERALL SUMMARY ===\")\n",
        "print(overall_summary_df.to_string(index=False))\n",
        "\n",
        "print(\"\\n=== LEVEL SUMMARY (head) ===\")\n",
        "print(level_summary_df.head(20).to_string(index=False))"
      ],
      "metadata": {
        "id": "GYw4LeGFW_Tj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 가설 4\n",
        "시대에 따라 흥행하는 장르가 다르다.(1990~2025년 *5년)"
      ],
      "metadata": {
        "id": "yfSfULGIXCcT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#한글 글씨 폰트 설치\n",
        "%%capture\n",
        "!sudo apt-get install -y fonts-nanum\n",
        "!sudo fc-cache -fv\n",
        "!rm ~/.cache/matplotlib -rf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.font_manager as fm\n",
        "fm.fontManager.addfont('/usr/share/fonts/truetype/nanum/NanumGothic.ttf')\n",
        "plt.rcParams['font.family'] = 'NanumGothic'\n",
        "\n",
        "# 표에서 ('-') 마이너스 표시\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "# 필요한 라이브러리 임포트\n",
        "import pandas as pd # 데이터 분석 라이브러리\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt # 시각화 도구 라이브러리1\n",
        "import seaborn as sns # 시각화 도구 라이브러리2\n",
        "\n",
        "df = pd.read_csv(\"/content/movies_genres_ohe.csv\", on_bad_lines='skip')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import shapiro, f_oneway, kruskal\n",
        "\n",
        "df.columns = df.columns.str.strip()\n",
        "\n",
        "df[\"release_date\"] = pd.to_datetime(df[\"release_date\"], errors=\"coerce\")\n",
        "df[\"year\"] = df[\"release_date\"].dt.year\n",
        "\n",
        "df = df[(df[\"year\"] >= 1990) & (df[\"year\"] <= 2025)].copy()\n",
        "\n",
        "\n",
        "bins   = list(range(1990, 2030, 5))\n",
        "labels = [f\"{y}–{y+4}\" for y in bins[:-1]]\n",
        "df[\"period\"] = pd.cut(df[\"year\"], bins=bins, labels=labels, right=True, include_lowest=True)\n",
        "\n",
        "\n",
        "cols = df.loc[:, \"Action\":\"Western\"].columns.tolist()\n",
        "\n",
        "col_counts = df[cols].sum().sort_values(ascending=False)\n",
        "print(\"[1990~2025] 장르별 전체 편수:\\n\", col_counts, \"\\n\")\n",
        "\n",
        "period_genre = {}\n",
        "for c in cols:\n",
        "    period_genre[c] = df.loc[df[c] == 1].groupby(\"period\")[\"y_result\"].mean()\n",
        "\n",
        "period_genre = pd.DataFrame(period_genre).sort_index()\n",
        "print(\"5년 단위 × 장르별 평균 y_result (1990~2025):\")\n",
        "print(period_genre, \"\\n\")\n",
        "\n",
        "\n",
        "from scipy.stats import shapiro, levene, f_oneway, kruskal\n",
        "\n",
        "\n",
        "print(\"[정규성 검정: Shapiro-Wilk] (1990~2025, 전체 장르)\")\n",
        "normality_results = {}\n",
        "for c in cols:  # cols = ['Action', ..., 'Western']\n",
        "    vals = df.loc[df[c] == 1, \"y_result\"].dropna()\n",
        "    if len(vals) >= 8:\n",
        "        stat, p = shapiro(vals)\n",
        "        normality_results[c] = (stat, p, len(vals))\n",
        "        print(f\"{c:15s} → W={stat:.4f}, p={p:.4e}, n={len(vals)}\")\n",
        "    else:\n",
        "        print(f\"{c:15s} → 샘플 부족 (n={len(vals)})\")\n",
        "\n",
        "groups = [df.loc[df[c] == 1, \"y_result\"].dropna() for c in cols if len(df.loc[df[c] == 1, \"y_result\"].dropna()) > 1]\n",
        "stat, p = levene(*groups)\n",
        "print(f\"\\n[등분산성 검정: Levene]\\nW={stat:.4f}, p={p:.4e}\")\n",
        "\n",
        "if all(pv > 0.05 for _, pv, _ in normality_results.values()):\n",
        "\n",
        "    f_stat, p_val = f_oneway(*groups)\n",
        "    print(f\"\\n[ANOVA 결과]\\nF={f_stat:.4f}, p={p_val:.4e}\")\n",
        "else:\n",
        "    h_stat, p_val = kruskal(*groups)\n",
        "    print(f\"\\n[Kruskal-Wallis 결과]\\nH={h_stat:.4f}, p={p_val:.4e}\")\n",
        "\n",
        "df[\"release_year\"] = pd.to_datetime(df[\"release_date\"], errors=\"coerce\").dt.year\n",
        "df = df[df[\"release_year\"].notna()].copy()\n",
        "df[\"release_year\"] = df[\"release_year\"].astype(int)\n",
        "\n",
        "df[\"year_bin\"] = (df[\"release_year\"] // 5) * 5\n",
        "\n",
        "\n",
        "df = df[df[\"year_bin\"] >= 1990].copy()\n",
        "df.loc[df[\"release_year\"] >= 2025, \"year_bin\"] = 2025\n",
        "\n",
        "\n",
        "cols = df.loc[:, \"Action\":\"Western\"].columns.tolist()\n",
        "\n",
        "latest_bin = df[\"year_bin\"].max()\n",
        "\n",
        "df_latest = df[df[\"year_bin\"] == latest_bin]\n",
        "\n",
        "\n",
        "genre_mean_latest = {\n",
        "    c: df_latest.loc[df_latest[c] == 1, \"y_result\"].mean() for c in cols\n",
        "}\n",
        "genre_mean_latest = pd.Series(genre_mean_latest).dropna().sort_values(ascending=False)\n",
        "\n",
        "top5 = genre_mean_latest.head(5).index.tolist()\n",
        "print(f\" {latest_bin}년대 기준 Top5 장르:\", top5)\n",
        "\n",
        "trend_top5 = {c: df.loc[df[c] == 1].groupby(\"year_bin\")[\"y_result\"].mean() for c in top5}\n",
        "trend_top5 = pd.DataFrame(trend_top5).sort_index()\n",
        "\n",
        "\n",
        "trend_top5_pct = trend_top5.div(trend_top5.sum(axis=1).replace(0, np.nan), axis=0) * 100\n",
        "\n",
        "\n",
        "ax = trend_top5_pct.plot(kind=\"bar\", stacked=True, figsize=(14,7), alpha=0.9)\n",
        "ax.set_title(f\"최신구간 기준 1990~{latest_bin} (5년 단위) Top5 장르 y_result 비율 (100%)\")\n",
        "ax.set_xlabel(\"Year Bin (5년 단위)\")\n",
        "ax.set_ylabel(\"비율 (%)\")\n",
        "ax.legend(title=\"장르\", bbox_to_anchor=(1.02, 1), loc=\"upper left\")\n",
        "\n",
        "\n",
        "for container in ax.containers:\n",
        "    labels = [f\"{h.get_height():.0f}%\" if h.get_height() >= 8 else \"\" for h in container]\n",
        "    ax.bar_label(container, labels=labels, label_type='center', fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "data = {\n",
        "    \"Year Bin\": [1990, 1995, 2000, 2005, 2010, 2015, 2020],\n",
        "    \"Horror\": [19, 16, 19, 18, 18, 20, 21],\n",
        "    \"Animation\": [16, 26, 23, 25, 22, 24, 21],\n",
        "    \"War\": [27, 18, 14, 16, 16, 14, 20],\n",
        "    \"Science Fiction\": [17, 19, 20, 17, 21, 20, 19],\n",
        "    \"Family\": [21, 21, 24, 24, 22, 22, 19]\n",
        "}\n",
        "df_plot = pd.DataFrame(data)\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "for genre in df_plot.columns[1:]:\n",
        "    plt.plot(df_plot[\"Year Bin\"], df_plot[genre], marker=\"o\", label=genre)\n",
        "\n",
        "    for x, y in zip(df_plot[\"Year Bin\"], df_plot[genre]):\n",
        "        plt.text(x, y+0.5, f\"{y}%\", ha=\"center\", va=\"bottom\", fontsize=9)\n",
        "\n",
        "plt.title(\"최신구간 기준 1990~2020 (5년 단위) Top5 장르 y_result 비율 (선그래프)\")\n",
        "plt.xlabel(\"Year Bin (5년 단위)\")\n",
        "plt.ylabel(\"비율 (%)\")\n",
        "plt.legend(title=\"장르\")\n",
        "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "df[\"release_date\"] = pd.to_datetime(df[\"release_date\"], errors=\"coerce\")\n",
        "df[\"year\"] = df[\"release_date\"].dt.year\n",
        "df = df[df[\"year\"] >= 1980].copy()\n",
        "\n",
        "bins = list(range(1980, 2025, 5))\n",
        "labels = [f\"{y}–{y+4}\" for y in bins[:-1]]\n",
        "df[\"five_year_bin\"] = pd.cut(df[\"year\"], bins=bins, labels=labels, right=True, include_lowest=True)\n",
        "\n",
        "genre_cols = [\n",
        "    'Action','Adventure','Animation','Comedy','Crime','Documentary','Drama','Family',\n",
        "    'Fantasy','History','Horror','Music','Mystery','Romance','Science Fiction',\n",
        "    'TV Movie','Thriller','War','Western'\n",
        "]\n",
        "genre_cols = [c for c in genre_cols if c in df.columns]\n",
        "\n",
        "genre_by_period = df.groupby(\"five_year_bin\")[genre_cols].sum()\n",
        "\n",
        "print(\"=== 시대 × 장르별 영화 편수 ===\")\n",
        "print(genre_by_period)\n",
        "\n",
        "df[\"release_date\"] = pd.to_datetime(df[\"release_date\"], errors=\"coerce\")\n",
        "df[\"year\"] = df[\"release_date\"].dt.year\n",
        "df = df[df[\"year\"] >= 1980].copy()\n",
        "\n",
        "bins   = list(range(1980, 2025, 5))\n",
        "labels = [f\"{y}–{y+4}\" for y in bins[:-1]]\n",
        "df[\"five_year_bin\"] = pd.cut(df[\"year\"], bins=bins, labels=labels, right=True, include_lowest=True)\n",
        "\n",
        "\n",
        "genre_cols = df.loc[:, \"Action\":\"Western\"].columns.tolist()\n",
        "\n",
        "count_tbl = df.groupby(\"five_year_bin\")[genre_cols].sum().reindex(labels).fillna(0).astype(int)\n",
        "\n",
        "\n",
        "first_seen = {}\n",
        "for g in genre_cols:\n",
        "    nz = np.where(count_tbl[g].values > 0)[0]\n",
        "    first_seen[g] = labels[nz[0]] if len(nz) else None\n",
        "\n",
        "plt.figure(figsize=(14,8))\n",
        "sns.heatmap(count_tbl.T, cmap=\"YlGnBu\", linewidths=.5, linecolor=\"lightgray\", cbar_kws={\"label\":\"편수\"})\n",
        "plt.title(\"시대(5년) × 장르별 영화 편수\")\n",
        "plt.xlabel(\"5년 구간\")\n",
        "plt.ylabel(\"장르\")\n",
        "\n",
        "for gi, g in enumerate(count_tbl.columns):\n",
        "    pass\n",
        "\n",
        "for gi, g in enumerate(count_tbl.columns):\n",
        "    pass\n",
        "\n",
        "\n",
        "M = count_tbl.T\n",
        "for gi, g in enumerate(M.index):\n",
        "    nz = np.where(M.loc[g].values > 0)[0]\n",
        "    if len(nz) > 0:\n",
        "        fi = nz[0]\n",
        "        plt.scatter(fi+0.5, gi+0.5, marker=\"*\", s=120, color=\"red\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"=== 장르별 최초 등장 구간 ===\")\n",
        "for g, f in first_seen.items():\n",
        "    print(f\"{g:15s}: {f}\")\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12,7))\n",
        "for col in count_tbl.columns:\n",
        "    plt.plot(count_tbl.index, count_tbl[col], marker=\"o\", label=col, alpha=0.7)\n",
        "\n",
        "plt.title(\"시대별 모든 장르 영화 편수 추세\")\n",
        "plt.xlabel(\"5년 구간\")\n",
        "plt.ylabel(\"편수\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def summarize_animation(df, start=1990, end=2025, step=5):\n",
        "    df = df.copy()\n",
        "\n",
        "    df[\"release_date\"] = pd.to_datetime(df[\"release_date\"], errors=\"coerce\")\n",
        "    df[\"year\"] = df[\"release_date\"].dt.year\n",
        "    df = df[(df[\"year\"] >= start) & (df[\"year\"] <= end)].copy()\n",
        "\n",
        "    bins   = list(range(start, end + step, step))\n",
        "    labels = [f\"{y}–{y+step-1}\" for y in bins[:-1]]\n",
        "    df[\"period\"] = pd.cut(df[\"year\"], bins=bins, labels=labels,\n",
        "                          include_lowest=True, right=True)\n",
        "\n",
        "    totals_cnt  = df.groupby(\"period\", observed=False).size().reindex(labels).astype(float)\n",
        "    totals_perf = df.groupby(\"period\", observed=False)[\"y_result\"].sum().reindex(labels).astype(float)\n",
        "\n",
        "    ani_cnt  = df.groupby(\"period\", observed=False)[\"Animation\"].sum().reindex(labels).fillna(0.0)\n",
        "    ani_perf = df.loc[df[\"Animation\"]==1].groupby(\"period\", observed=False)[\"y_result\"] \\\n",
        "                 .sum().reindex(labels).fillna(0.0)\n",
        "    ani_mean = df.loc[df[\"Animation\"]==1].groupby(\"period\", observed=False)[\"y_result\"] \\\n",
        "                 .mean().reindex(labels)\n",
        "\n",
        "    share_cnt  = ((ani_cnt  / totals_cnt.replace(0, np.nan))  * 100).fillna(0.0)\n",
        "    share_perf = ((ani_perf / totals_perf.replace(0, np.nan)) * 100).fillna(0.0)\n",
        "\n",
        "    excess_pp = (share_perf - share_cnt).fillna(0.0)\n",
        "\n",
        "    summary = pd.DataFrame({\n",
        "        \"count\": ani_cnt,\n",
        "        \"count_share_%\": share_cnt,\n",
        "        \"perf_sum\": ani_perf,\n",
        "        \"perf_share_%\": share_perf,\n",
        "        \"mean_y_result\": ani_mean,\n",
        "        \"excess_perf_pp\": excess_pp\n",
        "    }).reindex(labels).round(2)\n",
        "\n",
        "    return summary, labels\n",
        "\n",
        "\n",
        "anim_summary, labels = summarize_animation(df, start=1990, end=2025, step=5)\n",
        "print(anim_summary)\n",
        "\n",
        "plt.figure(figsize=(9,4))\n",
        "plt.bar(anim_summary.index.astype(str), anim_summary[\"count\"])\n",
        "plt.title(\"Animation 제작 편수 (5년 단위)\")\n",
        "plt.xlabel(\"Period\"); plt.ylabel(\"편수\")\n",
        "plt.xticks(rotation=45, ha=\"right\")\n",
        "plt.tight_layout(); plt.show()\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12,4))\n",
        "\n",
        "plt.plot(anim_summary.index.astype(str), anim_summary[\"count_share_%\"],\n",
        "         marker=\"o\", label=\"편수 점유율(%)\", color=\"C0\")\n",
        "for x, y in zip(anim_summary.index.astype(str), anim_summary[\"count_share_%\"]):\n",
        "    plt.text(x, y-0.6, f\"{y:.1f}%\", ha=\"center\", va=\"top\", fontsize=9, color=\"C0\")\n",
        "\n",
        "plt.plot(anim_summary.index.astype(str), anim_summary[\"perf_share_%\"],\n",
        "         marker=\"o\", label=\"성과 점유율(%)\", color=\"C1\")\n",
        "for x, y in zip(anim_summary.index.astype(str), anim_summary[\"perf_share_%\"]):\n",
        "    plt.text(x, y+0.6, f\"{y:.1f}%\", ha=\"center\", va=\"bottom\", fontsize=9, color=\"C1\")\n",
        "\n",
        "plt.title(\"Animation 점유율(%) — 편수 vs 성과\")\n",
        "plt.xlabel(\"Period\")\n",
        "plt.ylabel(\"%\")\n",
        "plt.legend()\n",
        "plt.xticks(rotation=45, ha=\"right\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(9,4))\n",
        "plt.bar(anim_summary.index.astype(str), anim_summary[\"perf_sum\"])\n",
        "plt.title(\"Animation 성과 합 (y_result)\")\n",
        "plt.xlabel(\"Period\"); plt.ylabel(\"합계 y_result\")\n",
        "plt.xticks(rotation=45, ha=\"right\")\n",
        "plt.tight_layout(); plt.show()\n",
        "\n",
        "xs = np.arange(len(anim_summary))\n",
        "y  = anim_summary[\"mean_y_result\"].values\n",
        "n  = anim_summary[\"count\"].values\n",
        "\n",
        "plt.figure(figsize=(9,4))\n",
        "plt.plot(xs, y, marker=\"o\")\n",
        "\n",
        "for xi, yi, ni in zip(xs, y, n):\n",
        "    if not np.isnan(yi):\n",
        "        plt.text(xi, yi, f\"n={int(ni)}\", ha=\"center\", va=\"bottom\", fontsize=9)\n",
        "\n",
        "plt.title(\"Animation 평균 y_result (5년 단위)\")\n",
        "plt.xlabel(\"Period\"); plt.ylabel(\"평균 y_result\")\n",
        "plt.xticks(xs, anim_summary.index.astype(str), rotation=45, ha=\"right\")\n",
        "plt.tight_layout(); plt.show()"
      ],
      "metadata": {
        "id": "AKEbibgIYIj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 가설 5\n",
        "장르별로 흥행 순위가 다를 것이다."
      ],
      "metadata": {
        "id": "x1c2A8buXFC0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## 추가 전처리\n",
        "\n",
        "# 1. get_dummies()를 사용하여 장르를 별도의 열로 분리\n",
        "# sep=', '는 쉼표와 공백을 기준으로 문자열을 나눈다는 의미입니다.\n",
        "genre_dummies = df['genres'].str.get_dummies(sep=', ')\n",
        "\n",
        "# 2. 기존 데이터프레임과 새로 생성된 장르 열을 합치기\n",
        "df = pd.concat([df, genre_dummies], axis=1)\n",
        "print(\"\\n--- 장르를 열로 확장한 데이터프레임 ---\")\n",
        "print(df)\n",
        "\n",
        "# CSV 파일로 저장\n",
        "df.to_csv(\"/content/movies_genres_ohe.csv\", index=False)\n",
        "print(\"CSV 파일 저장 완료: movies_genres_ohe.csv\")\n",
        "\n",
        "\n",
        "## 장르별 평균 y_result 순위\n",
        "\n",
        "# 1. 장르 컬럼만 자동으로 추출\n",
        "non_genre_cols = [\n",
        "    'id','title','vote_average','vote_count','status','release_date',\n",
        "    'revenue','runtime','adult','budget','popularity','keywords','y_result', 'SR'\n",
        "]\n",
        "genre_cols = [c for c in df.columns if c not in non_genre_cols]\n",
        "\n",
        "# 2. 각 장르별 평균 y_result 계산\n",
        "genre_means = {}\n",
        "for genre in genre_cols:\n",
        "    mask = df[genre] == 1\n",
        "    if mask.sum() > 0:\n",
        "        genre_means[genre] = df.loc[mask, 'y_result'].mean()\n",
        "\n",
        "# 3. 평균 y_result 높은 순으로 정렬\n",
        "genre_means = pd.Series(genre_means).sort_values(ascending=False)\n",
        "\n",
        "# 4. 결과 출력\n",
        "print(\"장르별 평균 y_result (흥행 점수):\")\n",
        "print(genre_means)\n",
        "\n",
        "# 5. 그래프로 시각화\n",
        "plt.figure(figsize=(12,6))\n",
        "genre_means.plot(kind='bar', color='skyblue')\n",
        "plt.title('Average y_result by Genre', fontsize=14)\n",
        "plt.ylabel('Average y_result', fontsize=12)\n",
        "plt.xlabel('genre')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "## 추론통계\n",
        "\n",
        "groups = []\n",
        "labels = []\n",
        "for genre in genre_cols:\n",
        "    mask = df[genre] == 1\n",
        "    if mask.sum() > 0:\n",
        "        groups.append(df.loc[mask, 'y_result'])\n",
        "        labels.append(genre)\n",
        "\n",
        "# 1. 정규성 검사 (Shapiro-Wilk)\n",
        "print(\"=== Shapiro-Wilk normality test per group ===\")\n",
        "for label, data in zip(labels, groups):\n",
        "    if len(data) >= 3:\n",
        "        stat, p = stats.shapiro(data)\n",
        "        print(f\"{label:20s} W-stat={stat:.3f}, p={p:.3f}\")\n",
        "    else:\n",
        "        print(f\"{label:20s} (Too few samples for Shapiro test)\")\n",
        "\n",
        "# 2. 등분산성 검사 (Levene test)\n",
        "stat, p = stats.levene(*groups)\n",
        "print(\"\\n=== Levene’s test for equal variances ===\")\n",
        "print(f\"Levene stat={stat:.3f}, p={p:.3f}\")\n",
        "\n",
        "h_stat, p_val = stats.kruskal(*groups)\n",
        "\n",
        "print(\"Kruskal–Wallis H-statistic:\", h_stat)\n",
        "print(\"p-value:\", p_val)\n",
        "\n",
        "# 3. Dunn's test\n",
        "\n",
        "!pip install scikit-posthocs\n",
        "\n",
        "import scikit_posthocs as sp\n",
        "import pandas as pd\n",
        "\n",
        "df_long = []\n",
        "\n",
        "for genre in genre_cols:\n",
        "    mask = df[genre] == 1\n",
        "    if mask.sum() > 0:\n",
        "        temp = df.loc[mask, ['y_result']].copy()\n",
        "        temp['genre'] = genre\n",
        "        df_long.append(temp)\n",
        "\n",
        "df_long = pd.concat(df_long, axis=0)\n",
        "\n",
        "# 4. Dunn’s test 수행 (p-value Holm 보정)\n",
        "dunn_results = sp.posthoc_dunn(df_long, val_col='y_result', group_col='genre', p_adjust='holm')\n",
        "\n",
        "print(dunn_results)\n",
        "\n",
        "\n",
        "## 장르 조합별 평균 y_result 순위\n",
        "\n",
        "# 장르 컬럼들\n",
        "non_genre_cols = [\n",
        "    'id','title','vote_average','vote_count','status','release_date',\n",
        "    'revenue','runtime','adult','budget','popularity','keywords','y_result', 'SR'\n",
        "]\n",
        "genre_cols = [c for c in df.columns if c not in non_genre_cols]\n",
        "\n",
        "# 장르 컬럼을 전부 0/1 정수로 변환\n",
        "for col in genre_cols:\n",
        "    df[col] = df[col].apply(lambda x: 1 if str(x).strip().lower() in ['1','true','yes'] else 0).astype(int)\n",
        "\n",
        "# 장르 조합 컬럼 생성\n",
        "from itertools import combinations\n",
        "\n",
        "for genre1, genre2 in combinations(genre_cols, 2):\n",
        "    combo_name = f\"{genre1}_{genre2}\"\n",
        "    df[combo_name] = df[genre1] * df[genre2]\n",
        "\n",
        "# 각 조합별 y_result 평균 계산\n",
        "results = []\n",
        "for genre1, genre2 in combinations(genre_cols, 2):\n",
        "    combo_name = f\"{genre1}_{genre2}\"\n",
        "    subset = df[df[combo_name] == 1]\n",
        "    if len(subset) > 0:\n",
        "        avg_y = subset['y_result'].mean()\n",
        "        results.append({\n",
        "            'Combination': combo_name,\n",
        "            'NumMovies': len(subset),\n",
        "            'Avg_y_result': avg_y\n",
        "        })\n",
        "\n",
        "combo_df = pd.DataFrame(results).sort_values('Avg_y_result', ascending=False)\n",
        "print(combo_df.head(20))\n",
        "\n",
        "# 장르1 & 장르2 조합별 Top10 그래프\n",
        "top10 = combo_df.head(10)\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.barplot(x='Avg_y_result', y='Combination', data=top10, palette='viridis')\n",
        "plt.title('Top 10 Genre and Combination Coefficients on Box Office')\n",
        "plt.xlabel('Coefficient')\n",
        "plt.ylabel('Genre / Genre Combination')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "8BcKD_HpXvEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 가설 6\n",
        "개봉 시기에 따른 흥행여부는 차이가 있을 것이다."
      ],
      "metadata": {
        "id": "INbPqAWXXUfg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. 'genres' 열의 문자열을 쉼표 기준으로 잘라 리스트로 변환\n",
        "#    이 결과를 바로 'genres_type'이라는 새 칼럼에 저장합니다.\n",
        "df['genres_type'] = df['genres'].str.split(', ')\n",
        "\n",
        "\n",
        "# 2. 새로 만든 'genres_type' 칼럼을 기준으로 explode를 적용\n",
        "#    이제 df의 행들이 'genres_type' 리스트의 각 요소에 따라 확장됩니다.\n",
        "df = df.explode('genres_type')\n",
        "\n",
        "\n",
        "df['release_date'] = pd.to_datetime(df['release_date'], errors='coerce')\n",
        "# 2. 날짜 형식으로 변환된 컬럼에서 '월'과 '분기' 정보 추출\n",
        "# .dt 접근자를 사용하면 날짜 관련 정보를 쉽게 뽑아낼 수 있습니다.\n",
        "df['month'] = df['release_date'].dt.month\n",
        "df['quarter'] = df['release_date'].dt.quarter\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# --- '월(month)'과 'Y Result' 간의 카이제곱 검정 ---\n",
        "\n",
        "print(\"--- '월(month)' 기준 카이제곱 검정 ---\")\n",
        "\n",
        "# 1. 교차표(Contingency Table) 생성\n",
        "# 행(index)에는 흥행 등급, 열(column)에는 개봉 월을 배치합니다.\n",
        "contingency_table_month = pd.crosstab(df['y_result'], df['month'])\n",
        "\n",
        "print(\"\\n[월별 흥행 등급 교차표]\")\n",
        "print(contingency_table_month)\n",
        "\n",
        "# 2. 카이제곱 검정 실행\n",
        "# 교차표를 chi2_contingency 함수에 전달합니다.\n",
        "chi2, p_value, dof, expected = chi2_contingency(contingency_table_month)\n",
        "\n",
        "print(f\"\\n카이제곱 통계량(Chi-squared Statistic): {chi2:.4f}\")\n",
        "print(f\"p-value: {p_value:.4f}\")\n",
        "print(f\"자유도(Degrees of Freedom): {dof}\")\n",
        "\n",
        "# 3. 결과 해석\n",
        "alpha = 0.05  # 유의수준 5%\n",
        "if p_value < alpha:\n",
        "    print(\"\\n[결론] p-value가 유의수준보다 작으므로, '개봉 월'과 '흥행'은 통계적으로 유의미한 관련이 있습니다.\")\n",
        "else:\n",
        "    print(\"\\n[결론] p-value가 유의수준보다 크므로, '개봉 월'과 '흥행'의 관련성을 통계적으로 입증하지 못했습니다.\")\n",
        "\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# 이전에 생성한 교차표 'contingency_table_month'가 있다고 가정합니다.\n",
        "# contingency_table_month = pd.crosstab(df['Y Result'], df['month'])\n",
        "\n",
        "# 1. 카이제곱 검정 재실행하여 '기대 빈도' 값 얻기\n",
        "chi2, p, dof, expected = chi2_contingency(contingency_table_month)\n",
        "\n",
        "# 2. 표준화 잔차 계산\n",
        "# 공식: (관측 빈도 - 기대 빈도) / sqrt(기대 빈도)\n",
        "residuals = (contingency_table_month - expected) / np.sqrt(expected)\n",
        "\n",
        "# 더 정확한 분석을 위해 조정 잔차(Adjusted Residuals)를 사용하는 경우도 많지만,\n",
        "# 해석의 편의성을 위해 표준화 잔차로도 충분히 의미있는 결과를 얻을 수 있습니다.\n",
        "# 여기서는 표준화 잔차(residuals)를 사용하겠습니다.\n",
        "\n",
        "print(\"--- 표준화 잔차 (Standardized Residuals) ---\")\n",
        "print(residuals)\n",
        "\n",
        "\n",
        "# 3. (선택사항) 결과를 보기 좋게 시각화하기 (강력 추천!)\n",
        "# 잔차 값이 1.96보다 크거나 -1.96보다 작은 셀을 하이라이트하여 보여줍니다.\n",
        "def highlight_significant_cells(val):\n",
        "    if val > 1.96:\n",
        "        return 'background-color: lightgreen' # 유의미하게 많음 (긍정적)\n",
        "    elif val < -1.96:\n",
        "        return 'background-color: lightcoral'  # 유의미하게 적음 (부정적)\n",
        "    else:\n",
        "        return ''\n",
        "\n",
        "styled_residuals = residuals.style.applymap(highlight_significant_cells)\n",
        "\n",
        "print(\"\\n\\n--- 유의미한 셀 하이라이트 ---\")\n",
        "# Jupyter Notebook이나 Colab 환경에서 실행하면 색상이 입혀진 표가 보입니다.\n",
        "display(styled_residuals)\n",
        "\n",
        "\n",
        "# 1. '성공 그룹' 데이터 필터링 (y_result가 2 또는 3)\n",
        "success_df = df[df['y_result'].isin([2, 3])].copy()\n",
        "\n",
        "# 2. 예산이 0보다 크고, 결측치가 없는 데이터만 남기기\n",
        "success_df = success_df[success_df['budget'] > 0].dropna()\n",
        "\n",
        "print(\"성공 그룹 데이터 준비 완료!\")\n",
        "print(f\"분석에 사용할 성공 영화 개수: {len(success_df)}\")\n",
        "\n",
        "\n",
        "from scipy.stats import ttest_ind\n",
        "\n",
        "# 2단계에서 생성한 success_df 사용\n",
        "\n",
        "# 1. '성수기'와 '비수기' 그룹으로 데이터 나누기\n",
        "peak_season_months = [5, 6, 7, 11, 12]\n",
        "peak_group = success_df[success_df['month'].isin(peak_season_months)]['budget']\n",
        "off_peak_group = success_df[~success_df['month'].isin(peak_season_months)]['budget']\n",
        "\n",
        "print(f\"성수기 성공작 평균 예산: ${peak_group.mean():,.2f}\")\n",
        "print(f\"비수기 성공작 평균 예산: ${off_peak_group.mean():,.2f}\")\n",
        "\n",
        "# 2. T-test 실행\n",
        "ttest_result = ttest_ind(peak_group, off_peak_group, equal_var=False)\n",
        "print(f\"\\nT-test p-value: {ttest_result.pvalue:.10f}\")\n",
        "\n",
        "# 3. 결과 해석\n",
        "if ttest_result.pvalue < 0.05:\n",
        "    print(\">> 결론: 성수기와 비수기 성공작들의 평균 예산 차이는 통계적으로 유의미합니다.\")\n",
        "else:\n",
        "    print(\">> 결론: 두 그룹의 평균 예산 차이는 통계적으로 유의미하지 않습니다.\")"
      ],
      "metadata": {
        "id": "EIJSvrTQi96x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 머신러닝 - 영화 예측 프로그램\n",
        "전처리과정"
      ],
      "metadata": {
        "id": "CHBtiPgwXcvn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 0. 라이브러리\n",
        "# ==============================================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from pathlib import Path\n",
        "from scipy import sparse\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. 데이터 로드 및 전처리\n",
        "# ==============================================================================\n",
        "CSV_PATH = \"/content/TMDB_processed_final.csv\"\n",
        "TARGET = \"y_result\"\n",
        "\n",
        "df = pd.read_csv(Path(CSV_PATH))\n",
        "\n",
        "# 타겟 0,1,2 -> 0, 3 -> 1\n",
        "y = df[TARGET].map(lambda v: 0 if v in [0,1] else 1).astype(int)\n",
        "X = df.drop(columns=[TARGET])\n",
        "\n",
        "# 날짜/수치형 파생\n",
        "X['release_date'] = pd.to_datetime(X['release_date'], errors='coerce')\n",
        "X['release_year'] = X['release_date'].dt.year\n",
        "X['release_month'] = X['release_date'].dt.month\n",
        "X['log_budget'] = np.log1p(X['budget'].clip(lower=0))\n",
        "\n",
        "# 텍스트 정제\n",
        "TEXT_COLS = [\"keywords\", \"overview\", \"tagline\"]\n",
        "def clean_text(s):\n",
        "    if pd.isna(s): return \"\"\n",
        "    s = str(s).lower().strip()\n",
        "    s = re.sub(r\"\\s*,\\s*\", \" \", s)\n",
        "    s = re.sub(r\"\\s+\", \" \", s)\n",
        "    return s\n",
        "\n",
        "for c in TEXT_COLS:\n",
        "    X[c] = X[c].apply(clean_text)\n",
        "\n",
        "# 텍스트 파생 피처\n",
        "for col in TEXT_COLS:\n",
        "    X[col + \"_len\"] = X[col].apply(lambda x: len(x))\n",
        "    X[col + \"_word_count\"] = X[col].apply(lambda x: len(x.split()))\n",
        "\n",
        "# 장르/감독 평균 점수 피처\n",
        "genre_avg = df.groupby(\"genres\")[TARGET].mean()\n",
        "director_avg = df.groupby(\"director\")[TARGET].mean()\n",
        "X[\"genre_avg_score\"] = X[\"genres\"].map(genre_avg).fillna(genre_avg.mean())\n",
        "X[\"director_avg_score\"] = X[\"director\"].map(director_avg).fillna(director_avg.mean())\n",
        "\n",
        "# 날짜 파생\n",
        "X['release_quarter'] = X['release_date'].dt.quarter.fillna(0)\n",
        "X['is_recent'] = X['release_year'].apply(lambda x: 1 if x >= 2015 else 0)\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. 학습/검증 분할\n",
        "# ==============================================================================\n",
        "RANDOM_STATE = 42\n",
        "TEST_SIZE = 0.2\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y\n",
        ")\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. TF-IDF 벡터화\n",
        "# ==============================================================================\n",
        "TFIDF_MAX_FEATURES = 20000\n",
        "X_train['text_combined'] = X_train[TEXT_COLS].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
        "X_valid['text_combined'] = X_valid[TEXT_COLS].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features=TFIDF_MAX_FEATURES, token_pattern=r\"[^ ]+\")\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train['text_combined'])\n",
        "X_valid_tfidf = vectorizer.transform(X_valid['text_combined'])\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. 수치형 + 파생 피처\n",
        "# ==============================================================================\n",
        "num_feature_cols = ['runtime','log_budget','release_year','release_month',\n",
        "                    'keywords_len','keywords_word_count',\n",
        "                    'overview_len','overview_word_count',\n",
        "                    'tagline_len','tagline_word_count',\n",
        "                    'genre_avg_score','director_avg_score',\n",
        "                    'release_quarter','is_recent']\n",
        "\n",
        "X_train_num = X_train[num_feature_cols].fillna(0)\n",
        "X_valid_num = X_valid[num_feature_cols].fillna(0)\n",
        "\n",
        "# ==============================================================================\n",
        "# 5. 최종 학습용 matrix 결합\n",
        "# ==============================================================================\n",
        "X_train_final = sparse.hstack([X_train_tfidf, sparse.csr_matrix(X_train_num.values)]).tocsr()\n",
        "X_valid_final = sparse.hstack([X_valid_tfidf, sparse.csr_matrix(X_valid_num.values)]).tocsr()\n",
        "\n",
        "# ==============================================================================\n",
        "# 6. SMOTE 적용\n",
        "# ==============================================================================\n",
        "smote = SMOTE(random_state=RANDOM_STATE)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_final, y_train)"
      ],
      "metadata": {
        "id": "eMiSFgoeZz-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델 학습"
      ],
      "metadata": {
        "id": "gFmK5a38Xhz0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 7. LightGBM 학습 (최적화 버전)\n",
        "# ==============================================================================\n",
        "lgb_model = lgb.LGBMClassifier(\n",
        "    boosting_type='gbdt',\n",
        "    objective='binary',\n",
        "    learning_rate=0.05,\n",
        "    n_estimators=500,\n",
        "    num_leaves=31,\n",
        "    max_depth=6,\n",
        "    feature_fraction=0.9,\n",
        "    bagging_fraction=0.8,\n",
        "    bagging_freq=5,\n",
        "    class_weight='balanced',\n",
        "    random_state=RANDOM_STATE,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "callbacks = [lgb.early_stopping(stopping_rounds=50), lgb.log_evaluation(100)]\n",
        "\n",
        "lgb_model.fit(\n",
        "    X_train_resampled, y_train_resampled,\n",
        "    eval_set=[(X_valid_final, y_valid)],\n",
        "    eval_metric='binary_logloss',\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "# ==============================================================================\n",
        "# 8. 평가\n",
        "# ==============================================================================\n",
        "y_pred = lgb_model.predict(X_valid_final)\n",
        "\n",
        "print(\"\\nAccuracy:\", accuracy_score(y_valid, y_pred))\n",
        "print(\"Precision:\", precision_score(y_valid, y_pred))\n",
        "print(\"Recall:\", recall_score(y_valid, y_pred))\n",
        "print(\"F1 Score:\", f1_score(y_valid, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_valid, y_pred))"
      ],
      "metadata": {
        "id": "OFLGuvXnXpul"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
